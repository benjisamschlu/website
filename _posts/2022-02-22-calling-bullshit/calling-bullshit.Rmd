---
title: "Calling bullshit"
description: |
  Post summarizing the course "Calling bullshit in the age of big data" which offers tools to enhance our reviewing skills.
author:
  - name: Benjamin Schl√ºter
date: 02-10-2022
output:
  distill::distill_article:
    self_contained: false
    highlight: monochrome
---

Carl Bergstrom and Jevin West wrote the book ["Calling Bullshit - the art of skepticism in a data-driven world"](https://www.penguinrandomhouse.com/books/563882/calling-bullshit-by-carl-t-bergstrom-and-jevin-d-west/) based on a course they taught in 2017, entitled ["Calling Bullshit in the age of big data" ](https://www.callingbullshit.org/videos.html). The aim of their course is to learn how to spot bullshit and effectively calling it, focusing on claims, not people. They are particularly seeing bullshit in the guise of big data and fancy algorithms which is the specific type of bullshit they are adressing in the course. 

The topics seemed to me of primary importance as a researcher for two reasons. First, I was seeing more and more "innovative machine learning" or "state-of-the art machine-learning methods" in proposals without further description. It is as if it was sufficient to invoke these two words to gain credibility. Second, in my opinion, reviewing is the central piece of research. The key element that gives us our credibility as a community. However, reviewing is not always easy due to all methodological complexity and rapid methodological developments. This course gives techniques and advises on how to spot inconsistencies, and hence, indirectly helps to become a better scientist/reviewer. Here below is a summary of points raised during their course that seemed important to me to remember as a researcher.



# An Introduction to Bullshit

According to Carl and Jevin, bullshit can be defined as follow:

*Bullshit involves language, statistical figures, data graphics, and other forms of presentation intended to impress, overwhelm, or persuade -presented with a blatant disregard for truth, logical coherence, or what information is actually being conveyed.*

Following this definition, Alberto Brandolini's Bullshit asymmetry principle is presented: 

*"The amount of energy necessary to refute bullshit is an order of magnitude bigger than to produce it."*

This statement resonates in me. A clear example in medical science is the case of Andrew Wakefield, a "researcher" that published in 1998 an article in the *Lancet*, making the link between vaccines and autism in children. Despite the fact that it has been shown that the paper was subject to numerous [limitations](https://www.bmj.com/content/342/bmj.c7452) (leading to its retraction from the journal), 24 years later, it is still an element raised by antivax and feeding the vaccine scare.



# Spotting Bullshit

They present a list of elements to remember in order to improve our skeptical mind:

* **If a claim seems too good/too bad to be true**

If an estimated effect size seems hughe, verify that the claim has been subject to rigorous analyses.

* **Beware of confirmation bias**

We tend to accept statements that support the views that we already have. Hence, we need to constantly challenge our views.

* **Multiple working hypotheses**

Take the habits of mind to come up with multiple alternative explanations for a claim. 

* **Think about orders of magnitude (Fermi estimation)**

When looking at numbers, try to put them in context to obtain an order of magnitude. This allows to have an informed judgement on a number.

* **Beware of unfair comparisons**

When things are compared, make sure that the comparison is not flawed. Things compared need to be of the same nature. Is the indicator- associated to each observation being compared- computed in the same way ?

* **Garbage in = garbage out**

Even when we don't know how an algorithm or statistical test works, we can spot bullshit by looking carefully at what goes in and what comes out. Is the data unbiased ? Is this data pertinent for answering a question ? Does the method tries to account for limitations in the data? Is the output making sense ? Does the output really support the claim ? Is the estimated effect size relevant according to what we are studying ?



# Correlation and causation


# Statistical traps and trickery

# Big data

# Data Visualization

# Publication bias

# Scholarly publishing and predatory publishers

# Fake news


# Refuting Bullshit
