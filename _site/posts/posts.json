[
  {
    "path": "posts/2022-02-11-what-is-the-likelihood/",
    "title": "You said the likelihood ?",
    "description": "Post explaining the likelihood, showing its derivation and its use in a demographic context.",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-11",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nIn this post, my aim is to explain what the likelihood is and describe how this concept is used in frequentist statistics to obtain an estimator for a parameter. At the end of the post, we will look at a demographic application.\r\nCoin flips\r\nLet’s assume that we are interested in coin flips. Hence, an observation (\\(y_i\\)) is the outcome of a coin flip and our data \\(\\{y_1, y_2, .., y_N\\}\\) consists of all \\(N\\) coin flips. First thing to do is to assume a distribution for the outcome of a coin flip. The commonly used distribution for a binary observation \\(y_i\\) taking only the following two values\r\n\\[\\begin{equation}\r\n  y_{i} =\r\n    \\begin{cases}\r\n      1 & \\text{if head}\\\\\r\n      0 & \\text{if tail}\r\n    \\end{cases}       \r\n\\end{equation}\\]\r\nis the Bernoulli distribution:\r\n\\[\r\n\\begin{align}\r\ny_i & \\sim Bernoulli(p) \\\\\r\n\\Leftrightarrow P(y_i) &  = p^{y_i}(1-p)^{1-y_i} = \\begin{cases} \r\np & \\text{if $y_i$ = 1} \\\\\r\n(1-p) & \\text{if $y_i$ = 0} \\end{cases}\r\n\\end{align}\r\n\\]\r\nwhere \\(p\\) represents the probability of observing head.\r\nOk that’s fine but we are working with a data set and thus, N coin flips. What is the probability of observing the N coin flips we obtained ? We can easily say that each coin flip is independent and since the coin used is always the same, all flips follow the same identical Bernoulli distribution (the famous i.i.d). These two assumptions are critical (hence their frequent apperearances in stat classes) and allow to express the probability of observing the data as the product of the probability of each coin flip:\r\n\\[\r\n\\begin{align}\r\nP(data) & = \\prod^N_{i=1} P(y_i) \\\\\r\n& = \\prod^N_{i=1} p^{y_i}(1-p)^{1-y_i} \\\\\r\n& = p^{\\sum^N_{i=1} y_i}(1-p)^{\\sum^N_{i=1} 1-y_i} \\\\\r\n& = p^{Y}(1-p)^{N-Y} \r\n\\end{align}\r\n\\] We need to multiply the above expression by \\(\\binom{N}{Y}\\) to account for the fact that Y heads could have been observed in different order out of N coin flips. The obtained quantity is the likelihood:\r\n\\[L(p|Y,N) = \\binom{N}{Y}p^Y(1-p)^{N-Y}\\]\r\n\\(L(p|Y,N)\\) is a function of a parameter \\(p\\) (the probability of heads) given \\(Y\\) and \\(N\\) (the data). It represents how likely it is to observe the actual data set according to the possible \\(p\\) values. Let’s check that visually by assuming that our data consists of 22 heads out of 60 flips.\r\n\r\n\r\n\r\nAs it is clear from the above figure, the likelihood is a function of the parameter \\(p\\) (bounded between 0 and 1 as it is a probability) given the observed data. Despite the fact that it looks like a probability distribution, it is not. The area under the likelihood curve does not sum to one.\r\nNow suppose, as it is commonly the case when performing analyses, that the parameter of interest is not known (here \\(p\\)). The way that frequentist statistics gets an estimator for \\(p\\) is by maximizing the probability of observing the data we have. Which, you now know at this stage of the post, is the likelihood. The obtained estimator is called the maximum likelihood estimator (MLE). Looking at the figure, the highest value reached by \\(L(p|Y,N)\\) seems to be around \\(p=0.37\\). In order to obtain its precise value, let’s find the argument (the value of \\(p\\)) that maximizes \\(L(p|Y,N)\\)\r\n\\[p^{MLE} = argmax_p(p^Y(1-p)^{N-Y})\\]\r\nThe above expression does not contain \\(\\binom{N}{Y}\\) as it does not depend on \\(p\\) and hence, will not play a role in the optimization. To find the maximum of a function, we take its derivative with respect to the parameter of interest and set it to 0. In practice, it is easier to first transform the likelihood into a log-likelihood \\(l(p|Y,N)\\) (monotonic transformation) and then find the value of the parameter maximizing it. Let’s do that,\r\n\\[\r\n\\begin{align}\r\nl(p|Y,N) = log(L(p|Y,N)) & = log(p^Y(1-p)^{N-Y}) \\\\\r\n\\Leftrightarrow  & = Ylog(p) + (N-Y)log(1-p) \r\n\\end{align}\r\n\\]\r\nTaking the derivative and setting it to zero to find the maximum,\r\n\\[\r\n\\begin{align}\r\n& \\frac{\\partial l(p|Y,N)}{\\partial p} = \\frac{Y}{p} - \\frac{N-Y}{1-p} = 0 \\\\\r\n& \\Leftrightarrow Y(1-p) - (N-Y)p = 0 \\\\\r\n& \\Rightarrow p^{MLE} = \\frac{Y}{N} = \\frac{22}{60} = 0.366\r\n\\end{align}\r\n\\]\r\nHaving the actual data set and willing to estimate the probability of obtaining head, the answer given by the maximum likelihood machinery is \\(p^{MLE}=0.366\\).\r\nThe beauty of this statistical machinery is that it can be applied to more complex examples. Let’s now make the link with models of mortality to conclude this post.\r\nDeaths at age \\(x\\)\r\nIn demography, we commonly assume that deaths at age \\(x\\) (\\(d_x\\)) are Poisson distributed\r\n\\[d_x \\sim Poisson(e_x \\mu_x)\\]\r\nwhere \\(e_x\\) is the number of person at risk of dying at age \\(x\\) and \\(\\mu_x\\) is the force of mortality. Hence, from this assumption, the probability that deaths at age \\(x\\) equal \\(d_x\\) can be expressed as follow\r\n\\[P(deaths~at~age~x=d_x) = \\frac{(e_x \\mu_x)^{d_x}e^{-(e_x \\mu_x)}}{d_x!}\\]\r\nIn general, mortality models assume a parametric form for \\(\\mu_x\\).\r\nHow does the likelihood help us here ? It allows us to estimate the parameters of these mortality models. Let’s repeat exactly the same steps as before. We already defined the distribution of the outcome of interest, deaths at age \\(x\\). This step corresponds to the coin flips example where we said that the outcome of a coin flip had a Bernoulli distribution. However, as before, we are interested in all our data (deaths at all ages). We know that the likelihood is the product of the probability of death at all ages (again assuming i.i.d). Thus, we can write the likelihood of our data as\r\n\\[L(\\mu_x|d_x, e_x) = \\prod^A_{x=0} \\frac{(e_x \\mu_x)^{d_x}e^{-(e_x \\mu_x)}}{d_x!}\\]\r\nwhere \\(A\\) is the highest age considered in our data set. Until here, we did not define any mortality model. Let’s do that ! In the Gompertz model, the force of mortality is expressed as\r\n\\[\\mu_x = ae^{bx}\\] Hence, the likelihood is obtained by replacing \\(\\mu_x\\) by the above expression in \\(L(\\mu_x|d_x, e_x)\\):\r\n\\[L(a, b|d_x, e_x) = \\prod^A_{x=0} \\frac{(e_x ae^{bx})^{d_x}e^{-(e_x ae^{bx})}}{d_x!}\\]\r\nThe only thing that remains to be done in order to obtain an estimated value for \\(a\\) and \\(b\\) is to find their values maximizing the above likelihood. In comparison to the coin flip example, even after taking the logarithm of \\(L(a, b|d_x, e_x)\\), we can’t obtain an analytical solution for \\(a\\) and \\(b\\) that would maximize the log-likelihood. In this context, we have to use a numerical method such as the Iteratively Reweighted Least-Squares (IRLS) algorithm but this goes outside the scope of this post …\r\nSummary\r\nThe likelihood is a way to obtain estimates for model’s parameters. Once we have assumed a distribution for our observations (i.e Poisson) and a mathematical expression of our model (ie Gompertz), it can easily be derived given that the i.i.d assumption is fulfilled. In practice, it is frequent that analytical solutions are not available when trying to maximize the likelihood. In such situations, we apply a numerical method (ie IRLS) on the log-likelihood to obtain estimates for parameters of interest.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-11-what-is-the-likelihood/what-is-the-likelihood_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-03-14T20:43:30+01:00",
    "input_file": "what-is-the-likelihood.knit.md"
  },
  {
    "path": "posts/2022-02-22-calling-bullshit/",
    "title": "Calling bullshit",
    "description": "Post summarizing the course \"Calling bullshit in the age of big data\" of interest when reviewing articles.",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\nCarl Bergstrom and Jevin West wrote the book “Calling Bullshit - the art of skepticism in a data-driven world” based on a course they taught in 2017, entitled “Calling Bullshit in the age of big data”. The aim of their course is to learn how to spot bullshit and effectively calling it, focusing on claims, not people. They are particularly seeing bullshit in the guise of big data and fancy algorithms which is the specific type of bullshit they are adressing in the course.\r\nThe topics seemed to me of primary importance as a researcher for two reasons. First, I was seeing more and more “innovative machine learning” or “state-of-the art machine-learning methods” in proposals without further description. It seems that it was sufficient to invoke these two words to gain credibility. Second, in my opinion, reviewing is one of the central piece of research. The key element that gives us our credibility as a community. However, reviewing is not always easy due to all methodological complexities and rapid methodological developments. Their course gives techniques and advises on how to spot inconsistencies, and hence, indirectly helps to become a better scientist/reviewer. Here below is a summary of points raised during their course that seemed important to me to share and to remember.\r\nAn Introduction to Bullshit\r\nAccording to Carl and Jevin, bullshit can be defined as follow:\r\n“Bullshit involves language, statistical figures, data graphics, and other forms of presentation intended to impress, overwhelm, or persuade -presented with a blatant disregard for truth, logical coherence, or what information is actually being conveyed.”\r\nFollowing this definition, Alberto Brandolini’s Bullshit asymmetry principle is presented:\r\n“The amount of energy necessary to refute bullshit is an order of magnitude bigger than to produce it.”\r\nThis statement resonates in me. A clear example in medical science is the case of Andrew Wakefield, a “researcher” that published in 1998 an article in the Lancet, making the link between vaccines and autism in children. Despite the fact that it has been shown that the paper was subject to numerous limitations (leading to its retraction from the journal), 24 years later, it is still an element raised by antivax and feeding the vaccine scare.\r\nSpotting Bullshit\r\nThey present a list of elements to remember in order to improve our skeptical mind:\r\nIf a claim seems too good/too bad to be true\r\nIf an estimated effect size seems hughe, verify that the claim has been subject to rigorous analyses.\r\nBeware of confirmation bias\r\nWe tend to accept statements that support the views that we already have. Hence, we need to constantly challenge our views.\r\nMultiple working hypotheses\r\nTake the habits of mind to come up with multiple alternative explanations for a claim.\r\nThink about orders of magnitude (Fermi estimation)\r\nWhen looking at numbers, try to put them in context to obtain an order of magnitude. This allows to have an informed judgement on a number.\r\nBeware of unfair comparisons\r\nWhen things are compared, make sure that the comparison is not flawed. Things compared need to be of the same nature. Is the indicator- associated to each observation being compared- computed in the same way ?\r\nGarbage in = garbage out\r\nEven when we don’t know how an algorithm or statistical test works, we can spot bullshit by looking carefully at what goes in and what comes out. Is the data unbiased ? Is this data pertinent for answering a question ? Does the method tries to account for limitations in the data? Is the output making sense ? Does the output really support the claim ? Is the estimated effect size relevant according to what we are studying ?\r\nCorrelation and causation (no super usefull)\r\nThey differentiate causation and correlation by giving their definitions:\r\n“Two variables are correlated when knowing the value of one gives you information about the likely value of the other.”\r\n“Two states are causally related when on state influences the other via a cause-and-effect process.”\r\nWhen looking at correlation, they encourage us to always ask ourselves, is it causation or coincidence (spurious correlation) ? If there is causation, which way does causality go? Is this causality direct or mediated by a common cause ?\r\nStatistical traps and trickery\r\nWhen we don’t know an algorithm or statistical test: what comes in and what comes out?\r\nRight censoring\r\nAverage caution: what is the underlying distribution, extreme values, type of data\r\nConvenience sampling\r\nP-values (the prosecutor’s fallacy) (4.3)\r\nBig data\r\nData exhaust: by-product of human activity\r\nScience methods is not dead\r\nMachines are not bias-free\r\nGarbage in, garbage out\r\nFallibility of machines\r\nAlgorithmic ethics\r\nBig data hubris: these methods should be complement not supplement\r\nBig data is not always better data\r\nOverfitting\r\nMachine learning: bias enter through training data Not knowing an algorithm, look at what goes in or not: is training data free of any bias?\r\nData Visualization\r\nPopular media do not frequently use multiple variables data visualizations. However, charts become more and more popular over time and they become more and more complicated. (Interaction with data visualization stick to your mind).\r\nMisleading axes: Truncating the y-axis (comparison and not starting at 0).\r\nManipulating bin sizes: completely change the figure we observe\r\nStyle don’t have to go in the way of information\r\nGlass Slippers: structure of data defines visualization’s form used. Fancy != information\r\nThe principle of proportional ink: “when a sheded …”\r\nPublication bias\r\nScholarly publishing and predatory publishers\r\nFake news\r\nRefuting Bullshit\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-03-12T16:18:21+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-02-reproducibility-in-data-centric-analysis/",
    "title": "Reproducibility in data-centric analyses",
    "description": "Post highlighting good practices for reproducibility presented during the DEMO council.",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nIn this post, I will summarise a talk I made to the members of my research center (DEMO), promoting reproducibility. I focused on computation reproducibility which requires that someone else should be able to reproduce your results with the materials (data, codes, guidance) you provided. This transaprency is necessary as our research results are used to inform public and health policies. The key elements I will go through should be seen as minimal requirements for reproducibility. I am sharing my own practice but there are of course other (better) ways to do.\r\nFolder’s structure\r\nFor someone else to be able to reproduce your analysis, a first requirement is that the repository associated to a research project is well organised. In addition, the project’s folder structure must permit an evolution over a long time period (sometimes really long ..). Hence, your desktop should definitely not look like this one (no judgment here):\r\n\r\n\r\n\r\nFigure 1: The don’t ..\r\n\r\n\r\n\r\nMy actual practice is to structure my project folder, here named “5-15 mortality estimation”, as follow\r\n\r\n\r\n\r\nFigure 2: .. and the do\r\n\r\n\r\n\r\nThis structure allows the project to evolve from data cleaning to submission, while maintaining a constant structure. At minimum, code has a subfolder called function, where I store R functions created for the project. I also add a stan subfolder for my Stan scripts. Each R code has a specific aim, for example, there will be one R code for data cleaning, one for exploratory data analysis (EDA) and so on. In the product folder, I usually store reports on EDA and manuscript drafts. I also write a mardown file for the project history. Note also that I locate my R project associated to that reseach project in the folder (bottom of the screen shot). It makes it easy to load data and functions, store data etc..\r\nProject history\r\nIn order to be transparent about the research process, I try to maintain a project history over the life of the project in the form of a markdown file (relevant format for the web). It contains the following elements:\r\nResearch questions\r\nMethodology\r\nData sources\r\nData cleaning & manipulations (with rationals)\r\nEarlier visualizations for reporting to colleagues\r\nData\r\n\r\n\r\n\r\nFigure 3: Was that version 37b’ ?\r\n\r\n\r\n\r\nI usually subdivide the data folder into raw and tidy. I do not save data version but rather the code performing the data cleaning and manipulation on the raw data. At the end of this code, I save the tidy data set in tidy. You can then load the data from this subfolder to save time if the cleaning code takes time to run.\r\nCoding practices\r\n\r\n\r\n\r\nFigure 4: What is this f@#&!% code doing?\r\n\r\n\r\n\r\nAlways remember Karl Broman’s quote: Your closest collaborator is you, six months ago, but you don’t respond to e-mails.\r\nCommenting your code and maintaining a project history may make you feel like losing time. My experience is that the time I “lose” is saved later on if I have to go back at the project, if I have to reuse part of a code or if I need to share it with a colleague. In my opinion, the time “lost” in a short term is more than saved over the long term.\r\nAs I already mentioned, I usually write one script per task\r\nCleaning\r\nEDA\r\nModeling\r\n..\r\nI also add a short description at the top of each script, write clear commenting and define sections in the script for better visibility.\r\n\r\n\r\n\r\nFigure 5: One example\r\n\r\n\r\n\r\nSharing code and data\r\n\r\n\r\n\r\nFigure 6: Version Control System (VCS)\r\n\r\n\r\n\r\nVersion control is a system that records changes to a file or set of files over time so that you can recall specific versions later. It also allows other to look at analyses you have performed and maybe, reproduce them.\r\nPersonally, I use Github which is free and is frequently used by the research community. Having an account allows you at least to\r\nShare your code online\r\nBack-up every version of your code\r\nCollaborate with colleagues (historically it was a tool for software developpers)\r\nIt requires to use the command line (Git Bash) but you only need some commands to start and maintain a project on your own.\r\nGithub in practice\r\nCreate an account on Github’s website.\r\nCreate a repository for a project (set a repo name, decide if public or private) and keep the obtained page open.\r\nDownload Git Bash on your computer\r\nOpen Git Bash and use the following commands (each one run by pressing enter):\r\n\r\n# Print the present working directory where Git Bash is located\r\npwd \r\n\r\n# Use cd \"repository path\" to let Git Bash know where to locate the current directory.\r\n# Locate it in the project you want to add to Github.  \r\ncd \"C:/Users/..../my_first_project\"\r\n\r\n# Initialize the folder as a local repo. Files in the folder will now be tracked.\r\ngit init    \r\n\r\n# Gives you the status (tracked/untracked) of the different folders and files present.\r\ngit status\r\n\r\n# Adding file(s)\r\ngit add \"filename.extension\"        \r\n# Alternatively, the command \"git add .\" will add all folders and files present in the repo.\r\n\r\n# Add a brief message explaining what have been the changes .\r\ngit commit -m \"your brief message here\"\r\n\r\n# Define that it is the main branch (easiest case)\r\ngit branch -M main\r\n\r\n# Connect your local repository to the one you created on the Github server.\r\n# This step is only required once when initializing a repo.\r\n# Note that the URL is shown at the top of the Github \r\n# webpage, just after creating your repository.\r\ngit remote add origin https://github.com/user_name/repo_name.git\r\n\r\n# Finaly, push the files and associated message from your laptop to Github\r\n# server.\r\n\r\ngit push -u origin main\r\n\r\nIf you now go back to your repository on Github and refresh the page, you should see the file(s) you added and the message associated to the changes.\r\nThe initialization is done. Let’s now focus on how it works when progressing on the project. Assume that you go back at this project some days later. Further suppose that you work on a script called “cleaning_data.R”. When you are satisfied with your work, open Git Bash and write the following command:\r\n\r\n# Running the command \"git status\" will tell you that cleaning_data.R\r\n# has been modified. However, this command is not needed for what follows.\r\n\r\n# Add the modifications performed on cleaning_data.R\r\ngit add \"cleaning_data.R\"\r\n\r\n# Add the message briefly describing what you changed in the script.\r\ngit commit -m \"convert NA into 0\"\r\n\r\n# Push the files to the Github server\r\ngit push -u origin main\r\n\r\nYour changes are now on the Github server. You only need to iterate these three commands each time you make progress on your project.\r\nThe explanation above is all what you need to start having your projects on Github. Some important notes:\r\nSuppose you have subfolders in your local repository. In order to add all folders and files to Github, you can simply use the command “git add .”. However, if you only want to add one file located in a subfolder, the file’s name needs to contain the path: git add \"./subfolder_name/file_name.extension\".\r\nIf you made you repository public, colleagues have the possibility to extract your repo on their laptop. This means more commands but opens the door to collaboration.\r\nSummary\r\nI presented some advises on how to conduct a research project while making it reproducible. I did not talk about pre-prints, open access journals and sharing data but these are also critical elements for reproducibility. In practice, it is frequently not as easy as presented in this post. Conducting research is more a circular process than a linear one. Nonetheless, keeping these elements in mind still allows to converge towards reproducibility in data-centric analyses.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-03-14T17:31:57+01:00",
    "input_file": "reproducibility-in-data-centric-analysis.knit.md"
  },
  {
    "path": "posts/2022-02-10-math-of-mortality/",
    "title": "Survival kit for survival analyses",
    "description": "Post on survival analyses deriving main mathematical expressions, simulating survival data applying the inverse CDF sampling with STAN and estimating a Piecewise Constant Hazards model.",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\nIn this post, I would like to introduce three complementary mathematical functions frequently encountered in survival analyses: the hazard, the survival and the age at death probability density function. Using STAN, we will then simulate survival data using the inverse cumulative density function (CDF) sampling method assuming the hazard has a Gompertz form. We conclude by estimating the hazard rate on our simulated dataset with a Piecewise Constant Hazard (PCH) model fitted with STAN.\r\nComplementary mathematical functions\r\nLet denote X: the age at death of an individual and assume that this random variable is \\(X\\geq 0\\) and continuous.\r\nSuriving beyond age x is expressed as\r\n\\[\r\n\\begin{align}\r\nS(x) & = Pr(X>x) \\\\\r\nS(x) & = 1 - F(x) \\\\\r\nS'(x) & = -f(x) \\\\\r\n\\Rightarrow S(x) & = -\\int^{\\infty}_xf(t)dt\r\n\\end{align}\r\n\\]\r\nLet’s now express the hazard rate, also called the force of mortality:\r\n\\[\r\n\\begin{align}\r\nh(x) &= lim_{\\Delta x \\rightarrow 0} \\frac{Pr(x \\leq X < x + \\Delta x | X > x)}{\\Delta x} \\\\\r\n &= lim_{\\Delta x \\rightarrow 0} \\frac{Pr(X < x + \\Delta x) - Pr(X \\leq x)}{\\Delta x} \\cdot \\frac{1}{Pr(X>x)} \\\\\r\n &= \\frac{f(x)}{S(x)}.\r\n\\end{align}\r\n\\]\r\nThus, the force of mortality can be expressed as\r\n\\[\r\n\\begin{align}\r\n\\boxed{h(x) = -\\frac{d}{dx}ln(S(x))}\r\n\\end{align}\r\n\\]\r\nand re-arranging the above equation, we express the survival probability at age x as\r\n\\[\\boxed{S(x) = e^{-\\int^x_0h(t)dt} = e^{-H(x)}}\\]\r\nwhere \\(H(x)\\) is called the cumulative hazard at age x.\r\nNote that when \\(h(x)\\) is constant,\r\n\\[S(x) = e^{-hx}.\\]\r\nFrom earlier expression of \\(h(x)\\), we can derive the age at death probability density function\r\n\\[\\boxed{f(x) = h(x)S(x)}.\\]\r\nThese formulas show that given either the hazard rate or the survival function, it is possible to get everything else.\r\nThe Gompertz case\r\nAssuming the hazard rate has the Gompertzian form\r\n\\[h(x) = ae^{bx}\\] we can easily derive the two other functions. We start by deriving the cumulative hazard function\r\n\\[\r\n\\begin{align}\r\nH(x) & = \\int^x_0h(t)dt \\\\\r\n& = \\int^x_0ae^{bt}dt \\\\\r\n& = \\frac{a}{b}[e^{bx} - 1]\r\n\\end{align}\r\n\\]\r\nand replace it in the survival function\r\n\\[\r\n\\begin{align}\r\nS(x) & = e^{-H(x)} \\\\\r\n& = e^{\\frac{a}{b}[1 - e^{bx}]}.\r\n\\end{align}\r\n\\]\r\nIt is then straightforward to obtain the death probability density function by multiplying both\r\n\\[\r\n\\begin{align}\r\nf(x) & = h(x)S(x) \\\\\r\n& = ae^{bx} e^{\\frac{a}{b}[1 - e^{bx}]} \r\n\\end{align}\r\n\\]\r\nSuppose now that we follow individuals from their 55\\(^{th}\\) birthdays to their death. Using the Human Mortality Database for Belgium in 2015, we estimated the Gompertz parameters to be \\(a = 0.0006\\) and \\(b = 0.128\\) on the ages 55 to 95 years old. Note that we restricted the estimation to these ages as the Gompertz model does not perform well for the young and oldest ages.\r\nWith this in hand, we can now plot our three complementary functions assuming Gompertz hazard and setting \\(a\\) and \\(b\\) to the values we estimated.\r\n\r\n\r\n\r\nSimulating survival data\r\nThe inverse CDF sampling method can be used to generate data. This method uses the fact that\r\n\\[ U = F(X) \\sim U_{[0,1]}\\]\r\nTherefore, if \\(u_1,...,u_n\\) is an indenpendent and identically distributed (iid) random sample from \\(U_{[0,1]}\\), then\r\n\\[\\{x_i:x_i = F^{-1}(u_i), i=1,...,n\\}\\] is an iid random sample from \\(F()\\). The algorithm depends on the feasibility to inverse \\(F()\\).\r\nApplying this method on the survival function, we can simulate ages at death\r\n\\[\r\n\\begin{align}\r\nF(x_i) & = u_i \\\\\r\nS(x_i) & = 1- u_i \\\\\r\ne^{\\frac{a}{b}[1 - e^{bx_i}]} &  = 1-u_i \\\\\r\n1 - e^{bx_i} & = \\frac{b}{a}log(1-u_i)  \\\\\r\nx_i & = \\frac{1}{b}log(1 - \\frac{b}{a}log(1-u_i))\r\n\\end{align}\r\n\\]\r\nwith \\(u_i \\sim U_{[0,1]}\\).\r\nThe corresponding STAN code is\r\n\r\nfunctions {\r\n  // Inverse survival function\r\n  real inv_survival(real u, real a, real b) {\r\n    return (1/b) * log(1 - ((b/a)*log(1 - u))) ;\r\n  }\r\n}\r\n\r\ndata {\r\n  int<lower=1> N;\r\n  real a;\r\n  real<lower=0> b;\r\n}\r\n\r\ngenerated quantities {\r\n  real<lower=0> t[N];\r\n  \r\n  for (n in 1:N) {\r\n    real u = uniform_rng(0, 1);\r\n    t[n] = inv_survival(u, a, b);\r\n  }\r\n}\r\n\r\nThe simulation could also easily be performed with base R with the following code\r\n\r\n\r\nu <- runif(N, 0, 1)\r\nt <- (1/b)*log(1-((b/a)*log(1-u)))\r\n\r\n\r\n\r\n\r\n\r\n\r\nWe can now compare the density of the simulated ages at death with the true density to see that our simulation worked fine.\r\n\r\n\r\n\r\nHazard estimation with a PCH model\r\nSuppose now that we observe the simulated ages at death, not knowing the true hazard rate. Despite the fact that age at death is continuous, we can divide the time into 1-year intervals and assume that the hazard is constant in each interval. We can then use a Piecewise Constant Hazards model to estimate the hazard in these intervals.\r\nThe following code transform the simulated age at death in continuous time, t, into counts of death and exposures at each age interval.\r\n\r\n\r\n# width of interval (1 year)\r\neps <- 1\r\n# containers\r\ndth <- c()\r\nexp <- c()\r\n# breaks for 1-year age interval\r\nbrks <- seq(0, ceiling(max(t)), eps)\r\nfor (i in 1:(length(brks)-1)){\r\n    # death counts by interval\r\n    d <- sum( (t >= brks[i] & t < brks[i+1]) )\r\n    \r\n    # time lived in interval by those dying\r\n    e_d <- t[t >= brks[i] & t < brks[i+1]] - brks[i]\r\n    # time lived in interval by those surviving\r\n    e <- sum(t >= brks[i+1])*eps + sum(e_d)\r\n    \r\n    dth <- c(dth, d)\r\n    exp <- c(exp, e)\r\n}\r\n# create data set\r\ndf.counts <- tibble(interval = head(brks, -1),\r\n               mid = head(brks, -1) + (eps/2),\r\n               age = mid + 55,\r\n               dth = dth,\r\n               exp = exp)\r\n\r\n\r\n\r\nThese counts data are plotted on the bar charts below\r\n\r\n\r\n\r\nAssuming that the hazard is constant in each interval\r\n\\[\r\nh(x) = \\lambda_x ~ \\forall x \\in [x, x+1) \r\n\\]\r\nwe can write the PCH model as follow\r\n\\[\r\nD_x \\sim Poisson(\\lambda_xE_x)\r\n\\] where \\(D_x\\), \\(E_x\\) and \\(\\lambda_x\\) are deaths count, exposure and hazard rate for age \\(x\\), respectively. The following STAN code estimates the model\r\n\r\n\r\n\r\n\r\ndata {\r\n  int<lower=0> A; // nber of intervals \r\n  vector<lower=0>[A] age; // age-55\r\n  vector<lower=0>[A] n; // exposure\r\n  int<lower=0> d[A]; // deaths\r\n}\r\nparameters {\r\n  real a;\r\n  real<lower=0> b;\r\n}\r\ntransformed parameters {\r\n  vector[A] eta = a + b*age; // linear predictor\r\n}\r\nmodel {\r\n  vector[A] log_lambda;\r\n  log_lambda = a + b*age + log(n);\r\n  // Likelihood\r\n  d ~ poisson_log(log_lambda);\r\n\r\n  // Priors\r\n  a ~ normal(0, 10);\r\n  b ~ normal(0, 10);\r\n}\r\n\r\nFinally, we extract the posterior draws of \\(\\lambda_x\\) and use them to get posterior draws of \\(S(x)\\) and \\(f(x)\\) using the math expressions presented earlier. Looking at the figures below, the PCH model is close to the truth with \\(N=2e4\\) simulated ages at death.\r\n\r\n\r\n\r\nSummary\r\nSurvival analyses mainly require three complementary mathematical expressions: the hazard rate, the survival function and the age at death probability density function. Once we have a given mathematical form for the hazard rate (ie Gompertz), the two other functions can be derived. In order to test models, the inverse CDF sampling method allows to simulate survival data. One model considered in this post is the PCH model that can be modelled in a Bayesian estimation framework within STAN. Note that we did not consider censoring or truncation but it will be addressed in a future post.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-10-math-of-mortality/math-of-mortality_files/figure-html5/plot_truth-1.png",
    "last_modified": "2022-03-14T17:44:51+01:00",
    "input_file": "math-of-mortality.knit.md"
  },
  {
    "path": "posts/2022-02-10-demographic-indicators-of-belgium/",
    "title": "Belgian demografie or démographie",
    "description": "Post presenting some demographic indicators of Belgium",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\nWhile doing my PhD, I realised that I was working on such specific things that I could not give basic demographic facts of Belgium when discussing with friends or relatives. Thus, I decided to create this post which looks at Belgium from a demographic perspective. Data sources used in this post are freely available from Statbel Open Data and the Human Mortality Database.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPopulation structure\r\nIn 1930, the belgian population consisted of 8 millions inhabitants. Ninty years later, belgian’s population reached 11.5 millions.\r\n\r\n\r\n\r\nFrom 1930 to 2020, we clearly see that the bottom of the population’s age pyramid has narrowed. In addition, the number of individuals aged more than 65 (above the dashed white line) experienced an important growth. The “holes” that you see on the sides of the pyramids (especially visible in 1960) are explained by the two European wars in 1914-18 and 1939-44.\r\n\r\n\r\n\r\n\r\n\r\n\r\nIn 2020, the median age of the belgian population is 41 years old which means that the number of individuals younger that age equals the number of individual older that age. The share of individuals aged more than 65 in the population equals 18.1%. An indicator frequently encountered is the age dependency ratio which is the ratio between the population not in the labor force (aged 0 to 14 and 65+) and those usually in the labor force (ages 15 to 64). It is a proxy to reflect the pressure on the productive population and it was equal to 0.56 in 2020.\r\n\r\n\r\n\r\nLet’s now look at the ratio of male versus female over all ages. Globally, there are 105 male births for 100 female births which explains why the ratio is close to 1.05 for the youngest age groups in Belgium. The ratio gets closer to 1 during the 30ies due to what is called the mortality hump (violent mortality), higher for young men. As we move to older age groups, the ratio converges quickly to zero due to a lower mortality for females in comparison to males.\r\n\r\n\r\n\r\nFertility\r\n\r\n\r\n\r\nThe figure below shows the daily number of births over the year 2019. There is substantial variation between week days and the week-end, in part explained by induced births and scheduled C-sections taking place during the week. It is quite shocking to observe how a natural process is structured by the medical profession. The average number of daily births during the week is 363.\r\n\r\n\r\n\r\n\r\n\r\n\r\nBy dividing the number of births by the number of women susceptible to have a birth at each age between 15 and 49 years old (reproductive ages), you obtain age-specific fertility rates (ASFR). They are presented for all reproductive ages in the figure below. The highest ASFR are at 29, 30 and 31 years old which means that the frequency of giving births for women at these ages is the highest. Summing these rates at all reproductive ages gives us the 2019 total fertility rate (TFR), equal to 1.6. It represents the number of children a woman would bear if she survived the reproductive ages, and experienced at each age the fertility rate of women of that age in 2019. It is important to realize that it consists of a hypothetical scenario which is noneless informative. The average age of mothers at the birth of their child is 31 in 2019.\r\n\r\n\r\n\r\nMortality\r\n\r\n\r\n\r\nIn the figure below, we show the daily number of deaths over the period 2016-2019. The average number of daily deaths in 2019 is equal to 298. There is however a high variation around that average due to a clear seasonal pattern of mortality. Indeed, daily death counts are higher in Winter respectively to Summer due to more cardio- and cerebro-vascular diseases and respiratory diseases.\r\n\r\n\r\n\r\n\r\n\r\n\r\nWhen studying mortality, age-specific mortality rates (ASMR) are equivalent to age-specific fertility rates in the context of fertility studies. For a given year, we divide the number of deaths by the number of person exposed to the risk of dying at each age. It is common to set a logarithmic scale to better visualize the changes over ages. We end up having a kind of inverse J-shape. Looking at the difference between curves for male and female allows to understand the evolution of the male to female ratio presented earlier.\r\n\r\n\r\n\r\nFinally, let’s look at the yearly life expectancy at birth over time in Belgium. As for the TFR, this is a hypothetical measure. It reflects the expected number of years an individual is expected to live, assuming that at each age, individuals face the mortality of the year considered. This means that in the year 1941, we suppose that individuals face the mortality observed during this year throughout their life. Hence, we assume that individuals live their entire life during the war taking place in the year 1941. This explains the big dip in life expectancy at birth for these years on the plot below. In 2019, life expectancy at birth is 79.6 and 84 for male and female, respectively.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-10-demographic-indicators-of-belgium/demographic-indicators-of-belgium_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-03-14T20:58:56+01:00",
    "input_file": "demographic-indicators-of-belgium.knit.md"
  },
  {
    "path": "posts/2022-02-10-temporal-smoother-with-bayes/",
    "title": "Easy smoothing with Bayes",
    "description": "Post demonstrating that Bayesian modeling makes it straightforward to smooth estimates.",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\nIn this post, I would like to show how convenient Bayesian modeling is for implementing smoothing. Note that this post has been inspired by the course “Bayesian subnational estimation using complex survey data” given by Jon Wakefield and Richard Li available online.\r\nPrevalence over time\r\nSuppose that we are interested in the population’s proportion having a specific characteric at a given time point (called the prevalence). Further assume that it would be too costly or too time consuming to survey all individuals in the population of interest at each time point. Hence, only a sample of the population is surveyed over 81 time points to assess the prevalence’s evolution over time.\r\nThe figure below shows points reflecting the sampled prevalence over time (using rbinom() function). The sample size is fixed at \\(n=100\\) and \\(n=1,000\\) on the left and right side, respectively. The dashed line represents the true population’s prevalence characterized by an exponential decay as time progresses.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe size of the sample has a big impact on the precision we have about the prevalence. This is clear from the picture with points much more spread around the true prevalence when \\(n=100\\) in comparison to \\(n=1000\\). In real life, due to constraints during the surveying process (time, budget,..), we can’t have hughe sample size and thus, we usually end up with prevalence estimates that might be varying over time, just due to the sampling variation (high point to point variation on the left figure). In this context, smoothing/penalization helps in estimating a quantity over time, when we expect that the true underlying prevalence in a population exhibits some degree of smoothness.\r\nBayesian formula and smoothing\r\nThe Bayes formula can be expressed as follow\r\n\\[p(\\theta|y) \\propto L(\\theta|y) \\times \\pi(\\theta)\\] where we have from left to right, the posterior distribution, the likelihood and the prior distribution. The likelihood describes the distribution of the data, depending on unknown parameters \\(\\theta\\) (see this post). The prior distribution expresses beliefs about \\(\\theta\\) and this beliefs can be expressed in such a way that they provide a mechanism by which smoothing can be imposed.\r\nModelling the prevalence\r\nIn our example, the likelihood should describe the distribution of a prevalence. It is common to model such type of variable with a logistic regression since the outcome variable is binary (an individual has a characteristic or not). This assumption leads to model the logit of \\(p\\) -where \\(p\\) is the prevalence we want to estimate- with a linear equation. Let’s write what we assumed so far\r\n\\[\r\n\\begin{align}\r\n& y_t|p_t \\sim Binomial(n, p_t) \\\\\r\n& log(\\frac{p_t}{1-p_t}) = a + \\phi_t \r\n\\end{align}\r\n\\]\r\nwhere \\(y_t\\) is the number of individuals having the characteristic at time \\(t\\) out of \\(n=100\\) sampled individuals (\\(n\\) fixed over time), \\(p_t\\) is the prevalence we want to estimate, \\(a\\) consists of an intercept and \\(\\phi_t\\) is a parameter that changes over time. Here comes the prior distribution as a mechanism to impose smoothing: we will assume that \\(\\phi_t\\) is distributed as a random walk of order one. This assumption encourages \\(\\phi_t\\) at \\(t\\) to be similar to its neighbors. The prior is expressed as\r\n\\[\r\n\\begin{align}\r\n\\pi(\\phi) & \\sim RW1 \\\\\r\n\\Leftrightarrow \\phi_t|\\phi_{t-1}, \\phi_{t+1}, \\sigma^2 & \\sim \\mathcal{N}(\\frac{1}{2}(\\phi_{t-1} + \\phi_{t+1}), \\frac{\\sigma^2}{2})\r\n\\end{align}\r\n\\]\r\nAccording to the selected prior distribution, values of \\(\\phi_t\\) close to \\(\\frac{1}{2}(\\phi_{t-1} + \\phi_{t+1})\\) are favored. It is clear from that distribution that \\(\\sigma\\) can be seen as a smoothing parameter since it defines the spread around \\(\\frac{1}{2}(\\phi_{t-1} + \\phi_{t+1})\\), which is the middle point between \\(\\phi_t\\)’s two neighbors. The figue below makes it clear that small (large) value of \\(\\sigma\\) enforces strong (weak) smoothing on \\(\\phi_t\\).\r\n\r\n\r\n\r\nLet’s now estimate this model on the sampled proportions using STAN\r\n\r\n\r\n\r\n\r\n# STAN code\r\ndata {\r\n  int<lower=0> T; // nber of time points\r\n  int<lower=0> n[T]; // sample size (fixed at 100)\r\n  int y[T]; // individual with disease\r\n}\r\nparameters {\r\n  real a;\r\n  vector[T] phi;\r\n  real<lower=0> sigma;\r\n}\r\ntransformed parameters {\r\n  vector[T] eta;\r\n\r\n  eta = a + phi;\r\n}\r\nmodel {\r\n  // Likelihood\r\n  y ~ binomial_logit(n, eta);\r\n  \r\n  // Priors\r\n  a ~ normal(0, 10);\r\n  \r\n  phi[1] ~ normal(0, sigma); // Random walk 1 for phi\r\n  phi[2:T] ~ normal(phi[1:(T-1)], sigma); // Random walk 1 for phi\r\n  sigma ~ normal(0.5, 0.05);\r\n}\r\ngenerated quantities {\r\n    vector[T] p_hat = 1 ./ (1+exp(-eta)); // estimated prevalence\r\n}\r\n\r\nIn the figure below we show the estimated posterior prevalence (with 95% credible interval) where we imposed different priors on \\(\\sigma\\) . On the left, we assumed that \\(\\sigma \\sim \\mathcal{N}^+(0.5,0.05)\\) while one the right, we imposed more smoothing by setting \\(\\sigma \\sim \\mathcal{N}^+(0,0.05)\\).\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe figure clearly shows that prior can be used to increase smoothing of our estimated posterior prevalence. Indeed, the right side of the figure, where we assumed that \\(\\sigma\\)’s distribution is centered on 0, shows much less wiggle than on the left.\r\nSummary\r\nIn Bayesian statistics, the posterior distribution of the parameters of interest is proportional to the product of the likelihood and the prior. In this post, it has been shown that the prior can be used as a mechanism to impose smoothing on the estimated quantity. This makes it straightforward to smooth estimates in a Bayesian estimation framework.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-10-temporal-smoother-with-bayes/temporal-smoother-with-bayes_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-03-14T21:16:17+01:00",
    "input_file": "temporal-smoother-with-bayes.knit.md"
  }
]
