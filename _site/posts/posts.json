[
  {
    "path": "posts/2022-02-11-what-is-the-likelihood/",
    "title": "You said the likelihood ?",
    "description": "Post opening the likelihood blackbox, showing its derivation and its use in a demographic context.",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-11",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nIn this post, my aim is to explain what the likelihood is and to describe how this concept is used in frequentist statistics to obtain an estimator for a parameter. At the end of the post, we will look at a demographic application.\r\nCoin flips\r\nLet’s assume that we are interested in coin flips. Hence, an observation (\\(y_i\\)) is the outcome of a coin flip and our data \\(\\{y_1, y_2, .., y_N\\}\\) consists of all \\(N\\) coin flips. First thing to do is to assume a distribution for the outcome of a coin flip. The commonly used distribution for a binary observation \\(y_i\\) taking only the following two values\r\n\\[\\begin{equation}\r\n  y_{i} =\r\n    \\begin{cases}\r\n      1 & \\text{if head}\\\\\r\n      0 & \\text{if tail}\r\n    \\end{cases}       \r\n\\end{equation}\\]\r\nis the Bernoulli distribution:\r\n\\[\r\n\\begin{align}\r\ny_i & \\sim Bernoulli(p) \\\\\r\n\\Leftrightarrow P(y_i) &  = p^{y_i}(1-p)^{1-y_i} = \\begin{cases} \r\np & \\text{if $y_i$ = 1} \\\\\r\n(1-p) & \\text{if $y_i$ = 0} \\end{cases}\r\n\\end{align}\r\n\\]\r\nwhere \\(p\\) represents the probability of observing head.\r\nOk that’s fine but we are working with a data set and hence, N flips. What is the probability of observing the data set we have ? We can easily say that each coin flip is independent and since the coin used is always the same, all flips follow the same identical Bernoulli distribution (the famous i.i.d). These two assumptions are critical (hence their frequent apperearances in stat classes) and allow to express the probability of observing the data as the product of the probability of each coin flip as follow:\r\n\\[\r\n\\begin{align}\r\nP(data) & = \\prod^N_{i=1} P(y_i) \\\\\r\n& = \\prod^N_{i=1} p^{y_i}(1-p)^{1-y_i} \\\\\r\n& = p^{\\sum^N_{i=1} y_i}(1-p)^{\\sum^N_{i=1} 1-y_i} \\\\\r\n& = p^{Y}(1-p)^{N-Y} \r\n\\end{align}\r\n\\] and this quantity is called, the likelihood:\r\n\\[L(p|Y) = p^Y(1-p)^{N-Y}\\]\r\n\\(L(p|Y)\\) is a function of a parameter \\(p\\) (the probability of heads) given \\(Y\\) (the data). Let’s check that visually by assuming that our data consists of 22 heads out of 60 flips.\r\n\r\n\r\n\r\nAs it is clear from the above figure, the likelihood is a function of the parameter \\(p\\) (bounded between 0 and 1 as it is a probability) given the observed data. Despite the fact that it looks like a probability distribution, it is not. The area under the likelihood curve does not sum to one.\r\nNow suppose, as it is commonly the case when performing analyses, that the parameter of interest is not known (here \\(p\\)). The way that frequentist statistics gets an estimator for \\(p\\) is by maximizing the probability of observing the data we have. Which, you now know at this stage of the post, is the likelihood. The obtained estimator is called the maximum likelihood estimator (MLE). Looking at the figure, the highest value reached by \\(L(p|Y)\\) seems to be around \\(p=0.37\\). In order to obtain its precise value, let’s find the argument (the value of \\(p\\)) that maximizes \\(L(p|Y)\\)\r\n\\[p^{MLE} = argmax_p(p^Y(1-p)^{N-Y})\\]\r\nAs it is usually done to find the maximum of a function, we take its derivative with respect to the parameter of interest and set it to 0. In practice, it is easier to first transform the likelihood into a log-likelihood \\(l(p|Y)\\) (monotonic transformation) and then find the value of the parameter maximizing it. Let’s do that,\r\n\\[\r\n\\begin{align}\r\nl(p|Y) = log(L(p|Y)) & = log(p^Y(1-p)^{N-Y}) \\\\\r\n\\Leftrightarrow  & = Ylog(p) + (N-Y)log(1-p) \r\n\\end{align}\r\n\\]\r\nTaking the derivative and setting it to zero to find the maximum,\r\n\\[\r\n\\begin{align}\r\n& \\frac{\\partial l(p|Y)}{\\partial p} = \\frac{Y}{p} - \\frac{N-Y}{1-p} = 0 \\\\\r\n& \\Leftrightarrow Y(1-p) - (N-Y)p = 0 \\\\\r\n& \\Rightarrow p^{MLE} = \\frac{Y}{N} = \\frac{22}{60} = 0.366\r\n\\end{align}\r\n\\]\r\nHaving the actual data set and willing to estimate the probability of obtaining head, the answer given by the maximum likelihood machinery is \\(p^{MLE}=0.366\\).\r\nThe beauty of this statistical machinery is that it can be applied to more complex examples. Let’s make the link with models of mortality to conclude this post.\r\nDeaths at age \\(x\\)\r\nIn demography, we commonly assume that deaths at age \\(x\\) (\\(d_x\\)) are Poisson distributed\r\n\\[d_x \\sim Poisson(e_x \\mu_x)\\]\r\nwhere \\(e_x\\) is the number of person exposed at age \\(x\\) and \\(\\mu_x\\) is the force of mortality. Hence, from this assumption, the probability that deaths at age \\(x\\) equal \\(d_x\\) can be expressed as follow\r\n\\[P(deaths=d_x) = \\frac{(e_x \\mu_x)^{d_x}e^{-(e_x \\mu_x)}}{d_x!}\\]\r\nIn general, mortality models assume a parametric form for \\(\\mu_x\\).\r\nHow does the likelihood help us here ? It allows us to estimate the parameters of these mortality models. Let’s repeat exactly the same steps as before. We already defined the distribution of the outcome of interest, deaths at age \\(x\\). This step corresponds to the coin flips example where we said that the outcome of a coin flip had a Bernoulli distribution. However, as before, we are interested in all our data (deaths at all ages). We know that the likelihood is the product of the probability of death at all ages (again assuming i.i.d). Thus, we can write the likelihood of our data as\r\n\\[L(\\mu_x|d_x, e_x) = \\prod^A_{x=0} \\frac{(e_x \\mu_x)^{d_x}e^{-(e_x \\mu_x)}}{d_x!}\\]\r\nwhere \\(A\\) is the highest age considered in our data set. Until here, we did not define any mortality model. Let’s do that ! In the Gompertz model, the force of mortality is expressed as\r\n\\[\\mu_x = ae^{bx}\\] Hence, the likelihood is obtained by replacing \\(\\mu_x\\) by the above expression in \\(L(\\mu_x|d_x, e_x)\\):\r\n\\[L(a, b|d_x, e_x) = \\prod^A_{x=0} \\frac{(e_x ae^{bx})^{d_x}e^{-(e_x ae^{bx})}}{d_x!}\\]\r\nThe only thing that remains to obtain an estimated value for \\(a\\) and \\(b\\) is to find their values maximizing the above likelihood. In comparison to the coin flip example, even after taking the logarithm of \\(L(a, b|d_x, e_x)\\), we can’t obtain an analytical solution for \\(a\\) and \\(b\\) that would maximize the log-likelihood. In this context, we have to use numerical method such as the Iteratively Reweighted Least-Squares (IRLS) algorithm but this goes outside the scope of this post!\r\nSummary\r\nThe likelihood is a way to otain estimates for model’s parameters. Once we have assumed a distribution for our observations (i.e Poisson) and a mathematical expression of our model (ie Gompertz), it can easily be derived given that the i.i.d assumption is fulfilled. In practice, it is frequent that analytical solutions are not available when trying to maximize the likelihood. In such situation, we apply numerical method (ie IRLS) on the log-likelihood to obtain parameters’ estimates.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-11-what-is-the-likelihood/what-is-the-likelihood_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-03-07T17:46:56+01:00",
    "input_file": "what-is-the-likelihood.knit.md"
  },
  {
    "path": "posts/2022-02-10-demographic-indicators-of-belgium/",
    "title": "Demographic indicators of Belgium",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\nPopulation evolution over time\r\nMedian age of Belgian\r\nPopulation pyramid on 3 time points\r\n+65 share variation across districts/commune?\r\nDependency ratio on the same time points\r\nFertility (age at first birth, ASFR, PASFR)\r\nSeasonality of death\r\nMale to Female ratio over age\r\nMortality ?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-10-demographic-indicators-of-belgium/demographic-indicators-of-belgium_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-02-26T22:34:22+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-10-ml-estimation-of-mortality-models-with-nn-r/",
    "title": "ML estimation of mortality models with NN-R",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\nWhen studying demographic methods for mortality estimations, we encounter the issue of estimating mortality at old ages where data are sparse (fewer deaths and fewer survivors). In this context, D. M. Feehan (2018) advised to estimate mortality models using maximum likelihood for several reasons. One of the reason that is of interest for this post is that estimating parameters by maximizing a likelihood accounts for the data available at each age, giving less influence to older ages in parameter estimates. In what follows, we will look at the difference between parameters estimated in the context of a simple linear model and parameters estimated in the context of a Maximum Likelihood estimation. The MLE will be performed using the Newton-Raphson (N-R) maximization algorithm to estimate parameters for the Gompertz and Log-Quadratic models.\r\nWe will use the Belgian mortality age schedule in 2002 (deaths and exposures from Human Mortality Database) where old age mortality showed “unusual” low mortality rates, most likely due to sparse data at these ages. What we will do in the following is to try to model data points in the rectangle showed on the figure.\r\n\r\n\r\n\r\n\r\n\r\n\r\nFitting mortality models by OLS\r\n\r\n\r\n\r\nOne solution sometimes performed (and that is not advisable as you will see) is to fit a linear model on the logarithm of the mortality rates. Gompertz and Log-Quadtric model would then be expressed respectively as follow:\r\n\\(\\begin{aligned}  ln(m_x) & \\sim Normal(\\alpha_1 + \\alpha_2x, \\sigma) \\\\  ln(m_x) & \\sim Normal(\\alpha_1 + \\alpha_2x + \\alpha_3x^2, \\sigma) \\end{aligned}\\)\r\nwhere \\(x \\in \\{80, 81, .., A\\}\\) is age from age 80 years old.\r\nPoisson Log Likelihood\r\nInstead we can derive the likelihood assuming deaths \\(D_x\\) follow a poisson distribution. We assume that deaths, \\(D_x\\), at each age \\(x \\in \\{80, 81, .., A\\}\\) have a Poisson distribution with expected value equal to the observed exposure \\(N_x\\) times the mortality rate:\r\n\\[D_x \\sim Poisson(N_x \\mu_x)\\] The sample likelihood can be expressed as\r\n\\[L(D_x | N_x, \\mu_x) = \\prod_{x=80}^G \\frac{(N_x \\mu_X)^{D_x}e^{-(N_x \\mu_x)}}{D_x!}\\]\r\nThus the sample log likelihood is\r\n\\[l(\\mu_x) = K + \\sum_{x=80}^G (D_xln(\\mu_x) - \\hat{D_x})\\]\r\nwhere \\(\\hat{D_x}=N_x \\mu_x\\) is the expected number of deaths predicted by the model, \\(K\\) is a constant not depending on \\(\\mu_x\\) and \\(G\\) corresponds to the number of age group considered.\r\nIn this post, we will consider two mortality models: Gompertz and Log-Quadratic. In what follows, we show the math for the Gompertz model. Mortality rates are commonly expressed as follow in the Gompertz model\r\n\\[\\mu_x = exp(\\alpha_1 + \\alpha_2x)\\] In matrix form we can rewrite that as\r\n\\[\\boldsymbol{\\lambda} = ln(\\boldsymbol{\\mu}) = \\boldsymbol{X\\alpha}\\]\r\nwhere \\(\\boldsymbol{X}\\) is a \\(G \\times 2\\) design matrix, \\(\\boldsymbol{\\alpha}\\) is a \\(2 \\times 1\\) vector and \\(ln(\\boldsymbol{\\mu})\\) is a \\(G \\times 1\\) vector of log mortality rates.\r\nMaximizing the Log Likelihood via Newton-Raphson iteration\r\nWe search \\(\\boldsymbol{\\alpha}\\) to maximize \\(\\boldsymbol{l(\\alpha)}\\). For Newton-Raphson algorithm to do that we need both first and second derivatives of \\(\\boldsymbol{l(\\alpha)}\\)\r\n\\[\\frac{\\partial \\boldsymbol{l(\\alpha)}}{ \\partial \\boldsymbol{\\alpha}} = \\sum_{x=80}^G(D_x-\\hat{D_x})\\boldsymbol{x_x} = \\boldsymbol{X'(D-\\hat{D})} = 0\\]\r\nsince \\(\\frac{ln(\\mu_x)}{\\partial \\boldsymbol{\\alpha}} = \\boldsymbol{x_x}\\), \\(\\frac{\\partial \\mu_x}{\\partial \\boldsymbol{\\alpha}} = \\mu_x \\boldsymbol{x_x}\\) and \\(\\frac{\\partial \\hat{D_x}}{\\partial \\boldsymbol{\\alpha}} = N_x\\mu_x \\boldsymbol{x_x} = \\hat{D_x}\\boldsymbol{x_x}\\).\r\nThe second derivatives are then\r\n\\[\\frac{\\partial^2 \\boldsymbol{l(\\alpha)}}{ \\partial \\boldsymbol{\\alpha}\\boldsymbol{\\alpha'}} = \\sum_{x=80}^G(-\\frac{\\partial \\hat{D_x}}{\\partial \\boldsymbol{\\alpha'}}\\boldsymbol{x_x}) = \\sum_{x=80}^G(\\boldsymbol{-x_x' \\hat{D_x} x_x}) = -\\boldsymbol{X'}diag(\\hat{\\boldsymbol{D}})\\boldsymbol{X}\\] The Newton-Raphson algorithm for solving \\(\\frac{\\partial \\boldsymbol{l(\\alpha)}}{ \\partial \\boldsymbol{\\alpha}}=0\\) is\r\n\\[\\boldsymbol{\\alpha_{i+1}} = \\boldsymbol{\\alpha_i}-[\\frac{\\partial^2 \\boldsymbol{l(\\alpha)}}{ \\partial \\boldsymbol{\\alpha}\\boldsymbol{\\alpha'}}]^{-1}[\\frac{\\partial \\boldsymbol{l(\\alpha)}}{ \\partial \\boldsymbol{\\alpha}}]\\] which in our case is\r\n\\[\\boldsymbol{\\alpha_{i+1}} = \\boldsymbol{\\alpha_i}+[\\boldsymbol{X'}diag(\\hat{\\boldsymbol{D}})\\boldsymbol{X}]^{-1}[\\boldsymbol{X'(D-\\hat{D})}]\\]\r\nand is shown in the R-code below\r\n\r\n\r\n# Design matrix\r\nX = matrix(c(rep(1, dim(df)[1]), df$Age),\r\n           ncol = 2)\r\n# vector of deaths and exposure\r\nD = df$D\r\nN = df$N\r\n# Expected deaths according to alpha\r\nDhat = function(alpha) {\r\n  lambda.hat = X %*% alpha\r\n  return( as.numeric( N * exp(lambda.hat)))\r\n}\r\n# Newton-Raphson algorithm\r\nnext_alpha = function(alpha) {\r\n  dhat = Dhat(alpha)\r\n  M = solve ( t(X) %*% diag(dhat) %*% X)\r\n  v = t(X) %*% (D - dhat)\r\n  return( alpha + M %*% v)\r\n}\r\n\r\na = matrix(0, 2, 15)\r\nfor (i in 2:ncol(a)) { a[,i] = next_alpha(a[,i-1])}\r\n\r\n\r\n\r\nAlpha values over N-R iterations. Convergence before tenth iteration.\r\n\r\n\r\n\r\nGompertz fit of old age mortality. Regression gives more weights to rates at oldest ages.\r\n\r\n\r\n\r\n\r\n\r\n\r\nAlgorithm only need a third column in design matrix and an additional line in a. Alpha values over N-R iterations. Convergence before tenth iteration.\r\n\r\n\r\n\r\nGompertz and Log-Quadratic fits. Regression outputs more sensitive to outlying value.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-10-ml-estimation-of-mortality-models-with-nn-r/ml-estimation-of-mortality-models-with-nn-r_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-02-11T12:01:15+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-10-temporal-smoother-with-bayes/",
    "title": "Easy smoothing with Bayes",
    "description": "Post demonstrating that Bayesian modeling makes it straightforward to implement smoothing.",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\nIn this post, I would like to show how convenient Bayesian modeling is for implementing smoothing. Note that this post has been inspired by the course “Bayesian subnational estimation using complex survey data” given by Jon Wakefield and Richard Li available online.\r\nPrevalence over time\r\nSuppose that we are interested in the population’s proportion having a specific characteric at a given time point (called the prevalence). Further assume that it would be too costly or too time consuming to survey all individuals in the population of interest at each time point. Hence, only a sample of the population is surveyed over 81 time points to assess the prevalence’s evolution over time.\r\nThe figure below shows points reflecting the sampled prevalence over time (using rbinom() function). The sample size is fixed at \\(n=100\\) and \\(n=1,000\\) on the left and right side, respectively. The dashed line represents the true population’s prevalence characterized by an exponential decay as time progresses.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe size of the sample has a big impact on the precision we have about the prevalence. This is clear from the picture with points much more spread around the true prevalence when \\(n=100\\) in comparison to \\(n=1000\\). In real life, due to constraints during the surveying process (time, budget,..), we can’t have hughe sample size and thus, we usually end up with prevalence estimates that might be varying over time, just due to the sampling variation (high point to point variation on the left figure). In this context, smoothing/penalization helps in estimating a quantity over time, when we expect that the true underlying prevalence in a population exhibits some degree of smoothness.\r\nBayesian formula and smoothing\r\nThe Bayes theorem can be expressed as follow\r\n\\[p(\\theta|y) \\propto L(\\theta|y) \\times \\pi(\\theta)\\] where we have from left to right, the posterior distribution, the likelihood and the prior distribution. The likelihood describes the distribution of the data, depending on unknown parameters \\(\\theta\\) (see this post). The prior distribution expresses beliefs about \\(\\theta\\) and this beliefs can be expressed in such a way that they provide a mechanism by which smoothing can be imposed.\r\nModelling the prevalence\r\nIn our example, the likelihood should describe the distribution of a prevalence. It is common to model such type of variable with a logistic regression since the outcome variable is binary (an individual has a characteristic or not). This assumption leads to model the logit of \\(p\\) -where \\(p\\) is the prevalence we want to estimate- with a linear equation. Let’s write what we assumed so far\r\n\\[\r\n\\begin{align}\r\n& y_t|p_t \\sim Binomial(n, p_t) \\\\\r\n& log(\\frac{p_t}{1-p_t}) = a + \\phi_t \r\n\\end{align}\r\n\\]\r\nwhere \\(y_t\\) is the number of individuals having the characteristic at time \\(t\\) out of \\(n=100\\) sampled individuals (\\(n\\) fixed over time), \\(p_t\\) is the prevalence we want to estimate, \\(a\\) consists of an intercept and \\(\\phi_t\\) is a parameter that changes over time. Here comes the prior distribution as a mechanism to impose smoothing: we will assume that \\(\\phi_t\\) is distributed as a random walk of order one. This assumption encourages \\(\\phi_t\\) at \\(t\\) to be similar to its neighbors. The prior is expressed as\r\n\\[\r\n\\begin{align}\r\n\\pi(\\phi) & \\sim RW1 \\\\\r\n\\Leftrightarrow \\phi_t|\\phi_{t-1}, \\phi_{t+1}, \\sigma^2 & \\sim \\mathcal{N}(\\frac{1}{2}(\\phi_{t-1} + \\phi_{t+1}), \\frac{\\sigma^2}{2})\r\n\\end{align}\r\n\\]\r\nAccording to the selected prior distribution, values of \\(\\phi_t\\) close to \\(\\frac{1}{2}(\\phi_{t-1} + \\phi_{t+1})\\) are favored. It is clear from that distribution that \\(\\sigma\\) can be seen as a smoothing parameter since it defines the spread around \\(\\frac{1}{2}(\\phi_{t-1} + \\phi_{t+1})\\), which is the middle point between \\(\\phi_t\\)’s two neighbors. The figue below makes it clear that small (large) value of \\(\\sigma\\) enforces strong (weak) smoothing on \\(\\phi_t\\).\r\n\r\n\r\n\r\nLet’s now estimate this model in STAN\r\n\r\n\r\n\r\n\r\n# STAN code\r\ndata {\r\n  int<lower=0> T; // nber of time points\r\n  int<lower=0> n[T]; // sample size (fixed at 100)\r\n  int y[T]; // individual with disease\r\n}\r\nparameters {\r\n  real a;\r\n  vector[T] phi;\r\n  real<lower=0> sigma;\r\n}\r\ntransformed parameters {\r\n  vector[T] eta;\r\n\r\n  eta = a + phi;\r\n}\r\nmodel {\r\n  // Likelihood\r\n  y ~ binomial_logit(n, eta);\r\n  \r\n  // Priors\r\n  a ~ normal(0, 10);\r\n  \r\n  phi[1] ~ normal(0, sigma); // Random walk 1 for phi\r\n  phi[2:T] ~ normal(phi[1:(T-1)], sigma); // Random walk 1 for phi\r\n  sigma ~ normal(0.5, 0.05);\r\n}\r\ngenerated quantities {\r\n    vector[T] p_hat = 1 ./ (1+exp(-eta)); // estimated prevalence\r\n}\r\n\r\nIn the figure below we show the estimated posterior prevalence (with 95% credible interval) where we imposed different priors on \\(\\sigma\\) . On the left, we assumed that \\(\\sigma \\sim \\mathcal{N}^+(0.5,0.05)\\) while one the right, we imposed more smoothing by setting \\(\\sigma \\sim \\mathcal{N}^+(0,0.05)\\).\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe figure clearly shows that prior can be used to increase smoothing of our estimated posterior prevalence. Indeed, the right side of the figure, where we assumed that \\(\\sigma\\)’s distribution is centered on 0, shows much less wiggle than on the left.\r\nSummary\r\nIn Bayesian statistics, the posterior distribution of the parameters of interest is proportional to the product of the likelihood and the prior. In this post, it has been showned that the prior can be used as a mechanism to impose smoothing on the estimated quantity. This makes it straightforward to smooth estimates in a Bayesian estimation framework.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-10-temporal-smoother-with-bayes/temporal-smoother-with-bayes_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-02-26T14:15:15+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-22-calling-bullshit/",
    "title": "Calling bullshit",
    "description": "Post summarizing the course \"Calling bullshit in the age of big data\" which offers tools to enhance our reviewing skills.",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\nCarl Bergstrom and Jevin West wrote the book “Calling Bullshit - the art of skepticism in a data-driven world” based on a course they taught in 2017, entitled “Calling Bullshit in the age of big data”. The aim of their course is to learn how to spot bullshit and effectively calling it, focusing on claims, not people. They are particularly seeing bullshit in the guise of big data and fancy algorithms which is the specific type of bullshit they are adressing in the course.\r\nThe topics seemed to me of primary importance as a researcher for two reasons. First, I was seeing more and more “innovative machine learning” or “state-of-the art machine-learning methods” in proposals without further description. It is as if it was sufficient to invoke these two words to gain credibility. Second, in my opinion, reviewing is the central piece of research. The key element that gives us our credibility as a community. However, reviewing is not always easy due to all methodological complexity and rapid methodological developments. This course gives techniques and advises on how to spot inconsistencies, and hence, indirectly helps to become a better scientist/reviewer. Here below is a summary of points raised during their course that seemed important to me to remember as a researcher.\r\nAn Introduction to Bullshit\r\nAccording to Carl and Jevin, bullshit can be defined as follow:\r\nBullshit involves language, statistical figures, data graphics, and other forms of presentation intended to impress, overwhelm, or persuade -presented with a blatant disregard for truth, logical coherence, or what information is actually being conveyed.\r\nFollowing this definition, Alberto Brandolini’s Bullshit asymmetry principle is presented:\r\n“The amount of energy necessary to refute bullshit is an order of magnitude bigger than to produce it.”\r\nThis statement resonates in me. A clear example in medical science is the case of Andrew Wakefield, a “researcher” that published in 1998 an article in the Lancet, making the link between vaccines and autism in children. Despite the fact that it has been shown that the paper was subject to numerous limitations (leading to its retraction from the journal), 24 years later, it is still an element raised by antivax and feeding the vaccine scare.\r\nSpotting Bullshit\r\nThey present a list of elements to remember in order to improve our skeptical mind:\r\nIf a claim seems too good/too bad to be true\r\nIf an estimated effect size seems hughe, verify that the claim has been subject to rigorous analyses.\r\nBeware of confirmation bias\r\nWe tend to accept statements that support the views that we already have. Hence, we need to constantly challenge our views.\r\nMultiple working hypotheses\r\nTake the habits of mind to come up with multiple alternative explanations for a claim.\r\nThink about orders of magnitude (Fermi estimation)\r\nWhen looking at numbers, try to put them in context to obtain an order of magnitude. This allows to have an informed judgement on a number.\r\nBeware of unfair comparisons\r\nWhen things are compared, make sure that the comparison is not flawed. Things compared need to be of the same nature. Is the indicator- associated to each observation being compared- computed in the same way ?\r\nGarbage in = garbage out\r\nEven when we don’t know how an algorithm or statistical test works, we can spot bullshit by looking carefully at what goes in and what comes out. Is the data unbiased ? Is this data pertinent for answering a question ? Does the method tries to account for limitations in the data? Is the output making sense ? Does the output really support the claim ? Is the estimated effect size relevant according to what we are studying ?\r\nCorrelation and causation\r\nStatistical traps and trickery\r\nBig data\r\nData Visualization\r\nPublication bias\r\nScholarly publishing and predatory publishers\r\nFake news\r\nRefuting Bullshit\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-03-03T14:54:44+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-10-math-of-mortality/",
    "title": "Math of mortality",
    "description": "Post showing the derivations of some mathematical identities used in mortality estimation with a simulation and estimation in STAN.",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\nThe field of mortality can be analyzed (at minimal) with three complementary functions: hazard, survival and death probability density function.\r\nLet denote X: the age at death of an individual and assume that this random variable is \\(X\\geq 0\\) and continuous.\r\nSuriving beyond age x is expressed as\r\n\\[\r\n\\begin{align}\r\nS(x) & = Pr(X>x) \\\\\r\nS(x) & = 1 - F(x) \\\\\r\nS'(x) & = -f(x) \r\n\\end{align}\r\n\\]\r\n\\[S(x) = Pr(X > x) = -\\int^{\\infty}_xf(t)dt\\]\r\nLet’s now express the hazard function, also called the force of mortality:\r\n\\[\r\n\\begin{align}\r\nh(x) &= lim_{\\Delta x \\rightarrow 0} \\frac{Pr(x \\leq X < x + \\Delta x | X > x)}{\\Delta x} \\\\\r\n &= lim_{\\Delta x \\rightarrow 0} \\frac{Pr(X < x + \\Delta x) - Pr(X \\leq x)}{\\Delta x} \\cdot \\frac{1}{Pr(X>x)} \\\\\r\n &= \\frac{f(t)}{S(t)}\r\n\\end{align}\r\n\\]\r\nThus, the force of mortality can be expressed as\r\n\\[\r\n\\begin{align}\r\n\\boxed{h(x) = -\\frac{d}{dx}ln(S(x))}\r\n\\end{align}\r\n\\]\r\nand re-arranging that equation we express the survival probability at age x as\r\n\\[\\boxed{S(x) = e^{-\\int^x_0h(t)dt} = e^{-H(x)}}\\]\r\nwhere \\(H(t)\\) is called the cumulative hazard.\r\nWe can further express \\(S(x+n)\\) as\r\n\\[S(x+n) = S(x)e^{-\\int^{x+n}_{x}h(t)dt}\\]\r\nNote that when \\(h(x)\\) is constant,\r\n\\[S(t) = e^{-ht}\\]\r\nFrom earlier derivation of \\(h(x)\\) we can obtain a math expression for the death probability density function\r\n\\[\\boxed{f(x) = h(x)S(x)}\\]\r\nLet’s check these identities visually or model a survival model with STAN ?\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-03-07T18:11:38+01:00",
    "input_file": "math-of-mortality.knit.md"
  },
  {
    "path": "posts/2022-03-02-reproducibility-in-data-centric-analysis/",
    "title": "Reproducibility in data-centric analyses",
    "description": "Post highlighting good practices for reproducibility presented during the DEMO council.",
    "author": [
      {
        "name": "Benjamin Schlüter",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\r\nIn this post, I will summarise a talk I made to the members of my research center (DEMO), promoting reproducibility. I focused on computation reproducibility which requires that someone else should be able to reproduce your results with the materials (data, codes, guidance) you provided. The key elements I will go through should be seen as minimal requirements for reproducibility. I am sharing my own practice but there are of course other (better) way to do.\r\nFolder’s structure\r\nFor someone else to be able to reproduce your analysis, a first requirement is that the repository associated to a research project is well organised. In addition, the project’s folder structure must permit an evolution over a long time period (sometimes really long ..). Hence, your desktop should definitely not look like this one (no judgment here):\r\n\r\n\r\n\r\nFigure 1: The don’t ..\r\n\r\n\r\n\r\nMy actual practice is to structure my project folder, here named “5-15 mortality estimation”, as follow\r\n\r\n\r\n\r\nFigure 2: .. and the do\r\n\r\n\r\n\r\nThis structure allows the project to evolve from data cleaning to submission, while maintaining a constant structure. At minimum, code has a subfolder called function, where I store R functions created for the project. I also add a stan subfolder for my Stan scripts. Each R code has a specific aim, for example, there will be one R code for data cleaning, one for exploratory data analysis (EDA) and so on. In the product folder, I usually store reports on EDA and manuscript drafts. I also write a mardown file for the project history. Note also that I locate my R project associated to that reseach in the folder (bottom of the screen shot).\r\nProject history\r\nIn order to be transparent about the research process, I try to maintain a project history over the life of the project in the form of a markdown file (easy formatting for the web). It contains the following elements:\r\nResearch questions\r\nMethodology\r\nData sources\r\nData cleaning & manipulations (with rationals)\r\nEarlier visualizations for reporting to colleagues\r\nData\r\n\r\n\r\n\r\nFigure 3: Was that version 37b’ ?\r\n\r\n\r\n\r\nI usually subdivide the data folder into raw and tidy. I do not save data version but rather the code performing the data cleaning and manipulation on the raw data. At the end of this code, I save the tidy data set in tidy. You can then load the data from this subfolder to save time if the cleaning code takes time to run.\r\nCoding practices\r\n\r\n\r\n\r\nFigure 4: What is this f@#&!% code doing?\r\n\r\n\r\n\r\nAlways remember Karl Broman’s quote: Your closest collaborator is you, six months ago, but you don’t respond to e-mails.\r\nCommenting your code and maintaining a project history may make you feel like losing time. My experience is that the time I “lose” is saved later on if I have to go back at the project, if I have to reuse part of a code or if I need to share it with a colleague. In my opinion, the time “lost” in a short term is more than saved over the long term.\r\nAs I already mentioned, I usually write one script per task\r\nCleaning\r\nEDA\r\nModeling\r\nFigures creation\r\n..\r\nI also add a short description at the top of each script, write clear commenting and define sections in the script for better visibility.\r\n\r\n\r\n\r\nFigure 5: One example\r\n\r\n\r\n\r\nSharing code and data\r\n\r\n\r\n\r\nFigure 6: Version Control System (VCS)\r\n\r\n\r\n\r\nVersion control is a system that records changes to a file or set of files over time so that you can recall specific versions later. It also allows other to look at analyses you have performed and maybe, reproduce them.\r\nPersonally, I use github which is free and is frequently used by the research community. Having an account allows you at least to\r\nShare your code online\r\nBack-up every version of your code\r\nCollaborate with colleagues (historically was a tool for software developpers)\r\nIt requires to use the command line (Git Bash) but you only need some commands to start and maintain a project on your own.\r\nGithub in practice\r\nCreate an account on Github’s website.\r\nCreate a repository for a project (set a repo name, decide if public or private) and keep the page open.\r\nDownload Git Bash on your computer\r\nOpen Git Bash and use the following commands (each one run by pressing enter):\r\n\r\n# Print the present working directory where Git Bash is located\r\npwd \r\n\r\n# Use cd \"repository path\" to let Git Bash know where to locate the current directory.\r\n# Locate it in the project you want to add to Github.  \r\ncd \"C:/Users/..../my_first_project\"\r\n\r\n# Initialize the folder as a local repo. Files in the folder will now be tracked.\r\ngit init    \r\n\r\n# Gives you the status (tracked/untracked) of the different folders and files present.\r\ngit status\r\n\r\n# Adding file(s)\r\ngit add \"filename.extension\"        \r\n# Alternatively, the command \"git add .\" will add all folders present in the repo.\r\n\r\n# Add a brief message explaining what have been the changes .\r\ngit commit -m \"your brief message here\"\r\n\r\n# Define that it is the main branch (easiest case)\r\ngit branch -M main\r\n\r\n# Connect your local repository to the one you created on the Github server.\r\n# This step is only required once when initializing a repo.\r\n# Note that the URL is shown at the top of the Github \r\n# webpage, just after creating your repository.\r\ngit remote add origin https://github.com/user_name/repo_name.git\r\n\r\n# Finaly, push the files and associated message from your laptop to Github\r\n# server.\r\n\r\ngit push -u origin main\r\n\r\nIf you now go back to your repository on Github and refresh the page, you should see the file(s) you added and the message associated to the changes.\r\nThe initialization is done. Let’s now focus on how it works when progressing on the project. Assume that you go back at this project some days later. Further suppose that you work on a script called “cleaning_data.R”. When you are satisfied with your work, open Git Bash and write the following command:\r\n\r\n# Running the command \"git status\" will tell you that cleaning_data.R\r\n# has been modified. However, this command is not needed right now.\r\n\r\n# Add the modifications performed on cleaning_data.R\r\ngit add \"cleaning_data.R\"\r\n\r\n# Add the message briefly describing what you changed in the script.\r\ngit commit -m \"convert NA into 0\"\r\n\r\n# Push the files to the Github server\r\ngit push -u origin main\r\n\r\nYour changes are now on the Github server. You only need to iterate these three commands each time you make progress on your project.\r\nThe explanation above is all what you need to start having your projects on Github. Some important notes:\r\nSuppose you have subfolders in your local repository. In order to add all folders and files to Github, you can simply use git add .. However, if you only want to add one file located in a subfolder, the file’s name needs to contain the path: git add \"./subfolder_name/file_name.extension\".\r\nIf you made you repository public, colleagues have the possibility to extract your repo on their laptop. This means more commands but opens the door to collaboration.\r\nSummary\r\nI presented some advises on how to conduct a research project while making it reproducible. In practice, it is frequently not as easy as presented in this post. Conducting research is more a circular process than a linear one. Nonetheless, keeping these elements in mind still allows to converge towards reproducibility in data-centric analyses.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-03-07T18:12:54+01:00",
    "input_file": "reproducibility-in-data-centric-analysis.knit.md"
  }
]
